# Recipa AI

A Retrieval-Augmented Generation (RAG) cooking assistant built with **LangChain**, **ChromaDB**, **FastAPI**, and **Next.js**.  
Ask grounded questions about recipes using a real cookbook PDF â€” with short-term chat memory for more natural interactions.

## ğŸ“Œ Overview

**Recipa AI** is a full-stack RAG project that answers cooking questions strictly using content from a cookbook PDF.  
It is built as a **portfolio-quality**, clean, modern example of:

- Retrieval-Augmented Generation (LangChain + Chroma)
- FastAPI backend with a `/ask` endpoint
- Next.js 14 frontend with Tailwind CSS
- Markdown-formatted AI responses
- Short-term chat memory (last 3 Q&A pairs)
- PDF ingestion â†’ vectorstore â†’ retrieval â†’ LLM answer

## ğŸ§  Architecture

```
PDF â†’ Text Splitter â†’ Embeddings â†’ ChromaDB (persisted)
                           â†“
                FastAPI `/ask` endpoint
                           â†“
       LangChain Prompt (with last 3 history items)
                           â†“
                     ChatOpenAI
                           â†“
                Markdown Answer â†’ Frontend UI
```

## âœ¨ Features

### ğŸ” Retrieval-Augmented Generation

- Uses **LangChain** and **ChromaDB** to return grounded answers
- Never hallucinates beyond the cookbook context

### âš¡ FastAPI Backend

- `/ask` endpoint with:
  - question
  - k (retrieval count)
  - history (last 3 Q&A pairs)

### ğŸ’¬ Short-Term Memory

The AI can resolve references like _â€œthe first oneâ€_ using the last **3** conversation turns.  
(Frontend memory = last 5, backend memory = last 3.)

### ğŸ–¥ï¸ Modern Next.js Frontend

- One-page UI (hero â†’ how-it-works â†’ chat â†’ team)
- Tailwind CSS
- Markdown rendering
- Clean and minimal cooking-themed styling

### ğŸ§© Deployment

- Frontend: **Vercel**
- Backend: **Render**

## ğŸ“¥ Project Structure

```
recipa-rag-assistant/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â””â”€â”€ rag/
â”‚   â”‚       â”œâ”€â”€ ingest.py
â”‚   â”‚       â”œâ”€â”€ retrieval.py
â”‚   â”‚       â””â”€â”€ llm.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ source/
â”‚   â”‚   â””â”€â”€ processed/
â”‚   â”œâ”€â”€ vectorstore/
â”‚   â”œâ”€â”€ scripts/run_ingest.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â””â”€â”€ frontend/
    â”œâ”€â”€ app/page.tsx
    â”œâ”€â”€ public/
    â”œâ”€â”€ components/
    â””â”€â”€ package.json
```

## ğŸš€ Getting Started

### 1ï¸âƒ£ Clone the Project

```bash
git clone https://github.com/WalidAlsafadi/recipa-rag-assistant
cd recipa-rag-assistant
```

## ğŸ Backend Setup (FastAPI)

### 2ï¸âƒ£ Create virtual environment

```bash
cd backend
python -m venv .venv
. .venv/bin/activate   # Windows: .venv\Scripts\activate
```

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Add environment variables

Create `backend/.env`:

```
OPENAI_API_KEY=your-key-here
```

### 5ï¸âƒ£ Ingest the cookbook PDF

```bash
python -m scripts.run_ingest
```

This creates `vectorstore/` with persisted embeddings.

### 6ï¸âƒ£ Start backend

```bash
uvicorn app.main:app --reload
```

Backend runs at:  
**http://localhost:8000**

## ğŸŒ Frontend Setup (Next.js 14)

### 1ï¸âƒ£ Install Node dependencies

```bash
cd frontend
npm install
```

### 2ï¸âƒ£ Add frontend environment variable

Create `frontend/.env`:

```
NEXT_PUBLIC_API_URL=http://localhost:8000
```

### 3ï¸âƒ£ Run frontend

```bash
npm run dev
```

Frontend runs at:  
**http://localhost:3000**

## ğŸ§‘â€ğŸ’» API (Minimal)

### **POST /ask**

Request:

```json
{
  "question": "How do I make the chocolate mug cake?",
  "k": 5,
  "history": [
    { "question": "Give me two dessert options", "answer": "1) ... 2) ..." }
  ]
}
```

Response:

```json
{
  "answer": "## Chocolate Mug Cake\n\n1. ..."
}
```

## ğŸ‘¥ Team

- **Walid Alsafadi**
- **Fares Alnamla**
- **Ahmed Alyazuri**

## ğŸ“„ License

This project is licensed under the **Apache License 2.0**, which requires attribution when used or modified.

See the full license below.

## ğŸ“¦ Future Improvements

- Add full chat memory
- Support multiple PDFs
- Add user authentication
- Add caching for repeated queries

Give us a â­ on GitHub if you like this project!
